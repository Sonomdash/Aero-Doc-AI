# Aero-Doc-AI: Chat Logic & RAG Pipeline –¢–∞–π–ª–±–∞—Ä

–≠–Ω—ç –±–∞—Ä–∏–º—Ç –Ω—å —Ç–∞–Ω—ã Aero-Doc-AI —Å–∏—Å—Ç–µ–º–∏–π–Ω —á–∞—Ç logic –±–æ–ª–æ–Ω RAG (Retrieval-Augmented Generation) pipeline-–∏–π–Ω “Ø–π–ª —è–≤—Ü—ã–≥ frontend-—ç—ç—Å —ç—Ö–ª—ç—ç–¥ backend, vector database —Ö“Ø—Ä—Ç—ç–ª –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç–∞–π–ª–±–∞—Ä–ª–∞–Ω–∞.

---

## üìã –ï—Ä”©–Ω—Ö–∏–π –¢–æ–π–º

Chat —Å–∏—Å—Ç–µ–º –Ω—å **RAG (Retrieval-Augmented Generation)** —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏ –∞—à–∏–≥–ª–∞–¥–∞–≥. –≠–Ω—ç –Ω—å:

1. **–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç** ‚Üí Embedding –±–æ–ª–≥–æ—Ö
2. **Vector Search** ‚Üí ChromaDB-—Å —Ö–æ–ª–±–æ–≥–¥–æ—Ö –±–∞—Ä–∏–º—Ç—ã–Ω —Ö—ç—Å–≥“Ø“Ø–¥–∏–π–≥ –æ–ª–æ—Ö
3. **Context Preparation** ‚Üí –û–ª–¥—Å–æ–Ω –º—ç–¥—ç—ç–ª–ª–∏–π–≥ –Ω—ç–≥—Ç–≥—ç—Ö
4. **LLM Generation** ‚Üí Groq (Llama 3) –∞—à–∏–≥–ª–∞–Ω —Ö–∞—Ä–∏—É–ª—Ç “Ø“Ø—Å–≥—ç—Ö
5. **Response Storage** ‚Üí –•–∞—Ä–∏—É–ª—Ç—ã–≥ database-–¥ —Ö–∞–¥–≥–∞–ª–∞—Ö

---

## üèó –°–∏—Å—Ç–µ–º–∏–π–Ω –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä

```mermaid
graph TB
    User[üë§ –•—ç—Ä—ç–≥–ª—ç–≥—á] --> Frontend[üé® Frontend ChatInterface]
    Frontend --> API[üåê Chat API Service]
    API --> Router[üîß Chat Router]
    Router --> ChatService[üíº Chat Service]
    
    ChatService --> DB[(üóÑÔ∏è PostgreSQL)]
    ChatService --> Embedder[ü§ñ Embeddings]
    ChatService --> VectorStore[üî¢ ChromaDB]
    ChatService --> LLM[üß† Groq Llama 3]
    
    DB --> Sessions[chat_sessions]
    DB --> Messages[chat_messages]
    
    VectorStore --> Chunks[Document Chunks]
    VectorStore --> Vectors[384-dim Vectors]
    
    style ChatService fill:#e1f5ff
    style VectorStore fill:#fff4e1
    style LLM fill:#ffe1f5
```

---

## üóÑÔ∏è Database Schema

### 1. `chat_sessions` Table

```sql
CREATE TABLE chat_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    title VARCHAR(255) DEFAULT 'New Chat',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**–•–∞–¥–≥–∞–ª–∞–≥–¥–∞—Ö –º—ç–¥—ç—ç–ª—ç–ª:**
- Session ID
- –•—ç—Ä—ç–≥–ª—ç–≥—á
- –ß–∞—Ç—ã–Ω –≥–∞—Ä—á–∏–≥ (—ç—Ö–Ω–∏–π –∞—Å—É—É–ª—Ç–∞–∞—Å “Ø“Ø—Å–≥—ç–Ω—ç)
- “Æ“Ø—Å–≥—ç—Å—ç–Ω/–®–∏–Ω—ç—á–∏–ª—Å—ç–Ω –æ–≥–Ω–æ–æ

### 2. `chat_messages` Table

```sql
CREATE TABLE chat_messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES chat_sessions(id) ON DELETE CASCADE,
    role VARCHAR(20) NOT NULL,  -- 'user' or 'assistant'
    content TEXT NOT NULL,
    sources JSONB,  -- Retrieved document chunks
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**–•–∞–¥–≥–∞–ª–∞–≥–¥–∞—Ö –º—ç–¥—ç—ç–ª—ç–ª:**
- Message ID
- Session ID
- Role: `user` (—Ö—ç—Ä—ç–≥–ª—ç–≥—á) —ç—Å–≤—ç–ª `assistant` (AI)
- Content: –ú–µ—Å—Å–µ–∂–∏–π–Ω –∞–≥—É—É–ª–≥–∞
- Sources: –û–ª–¥—Å–æ–Ω –±–∞—Ä–∏–º—Ç—ã–Ω —ç—Ö —Å—É—Ä–≤–∞–ª–∂—É—É–¥ (JSONB format)

**Sources JSONB Format:**
```json
[
  {
    "doc_id": "uuid",
    "filename": "manual.pdf",
    "chunk_index": 0,
    "page_number": 1
  }
]
```

---

## üîÑ –ë“Ø—Ç—ç–Ω Chat Flow

### Sequence Diagram

```mermaid
sequenceDiagram
    participant User as üë§ –•—ç—Ä—ç–≥–ª—ç–≥—á
    participant UI as üé® ChatInterface
    participant API as üåê API Service
    participant Router as üîß Chat Router
    participant Service as üíº Chat Service
    participant DB as üóÑÔ∏è PostgreSQL
    participant Embedder as ü§ñ Embeddings
    participant Vector as üî¢ ChromaDB
    participant LLM as üß† Groq Llama 3

    User->>UI: –ê—Å—É—É–ª—Ç –±–∏—á–∏—Ö
    UI->>UI: Optimistic UI update
    
    alt Session –±–∞–π—Ö–≥“Ø–π –±–æ–ª
        UI->>API: createSession(title)
        API->>Router: POST /api/chat/sessions
        Router->>Service: create_session()
        Service->>DB: INSERT chat_session
        DB-->>Service: session
        Service-->>Router: session
        Router-->>API: session
        API-->>UI: session
    end
    
    UI->>API: sendMessage(sessionId, content)
    API->>Router: POST /api/chat/sessions/{id}/messages
    Router->>Service: send_message()
    
    Service->>DB: Save user message
    DB-->>Service: user_msg saved
    
    Service->>Embedder: embed_query(content)
    Embedder-->>Service: query_embedding [384-dim]
    
    Service->>Vector: search(query_embedding, top_k=5)
    Vector-->>Service: {documents, metadatas, distances}
    
    Service->>Service: Format context from results
    
    Service->>LLM: invoke(system_prompt + context + question)
    LLM-->>Service: AI response
    
    Service->>DB: Save assistant message + sources
    DB-->>Service: assistant_msg saved
    
    Service->>DB: Update session title & timestamp
    
    Service-->>Router: assistant_msg
    Router-->>API: assistant_msg
    API-->>UI: assistant_msg
    UI-->>User: ‚úÖ –•–∞—Ä–∏—É–ª—Ç —Ö–∞—Ä—É—É–ª–∞—Ö
```

---

## üì± Frontend: Chat Interface

### –ë–∞–π—Ä—à–∏–ª
[ChatInterface.tsx](file:///Users/soonko/Documents/Dentsv/Aero-Doc-AI/frontend/src/components/Chat/ChatInterface.tsx)

### “Æ–Ω–¥—Å—ç–Ω –§—É–Ω–∫—Ü—É—É–¥

#### 1. Message Submit Handler

```typescript
const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim() || isLoading) return;

    const content = input.trim();
    setInput('');

    // Optimistic UI update - —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –º–µ—Å—Å–µ–∂–∏–π–≥ —à—É—É–¥ —Ö–∞—Ä—É—É–ª–∞—Ö
    const optimisticMessage: Message = {
        id: 'temp-' + Date.now(),
        session_id: session?.id || '',
        role: 'user',
        content: content,
        created_at: new Date().toISOString(),
    };

    setMessages(prev => [...prev, optimisticMessage]);
    setIsLoading(true);

    try {
        let currentSessionId = session?.id;

        // Session –±–∞–π—Ö–≥“Ø–π –±–æ–ª —à–∏–Ω—ç—ç—Ä “Ø“Ø—Å–≥—ç—Ö
        if (!currentSessionId) {
            const title = content.slice(0, 30) + '...';
            const newSession = await chatService.createSession(title);
            currentSessionId = newSession.id;
            if (onSessionCreated) onSessionCreated(newSession);
        }

        // –ú–µ—Å—Å–µ–∂ –∏–ª–≥—ç—ç—Ö
        const response = await chatService.sendMessage(currentSessionId, content);
        
        // AI —Ö–∞—Ä–∏—É–ª—Ç—ã–≥ –Ω—ç–º—ç—Ö
        setMessages(prev => [...prev, response]);

    } catch (error) {
        console.error('Failed to send message', error);
        alert('Failed to send message. Please try again.');
    } finally {
        setIsLoading(false);
    }
};
```

**–û–Ω—Ü–ª–æ–≥:**
- **Optimistic UI**: –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –º–µ—Å—Å–µ–∂–∏–π–≥ backend —Ö–∞—Ä–∏—É–ª–∞—Ö—ã–≥ —Ö“Ø–ª—ç—ç—Ö–≥“Ø–π–≥—ç—ç—Ä —à—É—É–¥ —Ö–∞—Ä—É—É–ª–Ω–∞
- **Auto Session Creation**: Session –±–∞–π—Ö–≥“Ø–π –±–æ–ª –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä “Ø“Ø—Å–≥—ç–Ω—ç
- **Auto Scroll**: –®–∏–Ω—ç –º–µ—Å—Å–µ–∂ –∏—Ä—ç—Ö –±“Ø—Ä–¥ –¥–æ–æ—à scroll —Ö–∏–π–Ω—ç

---

## üåê API Layer: Frontend Service

### –ë–∞–π—Ä—à–∏–ª
[chat.ts](file:///Users/soonko/Documents/Dentsv/Aero-Doc-AI/frontend/src/services/chat.ts)

### API Functions

```typescript
export const chatService = {
    // 1. –®–∏–Ω—ç session “Ø“Ø—Å–≥—ç—Ö
    async createSession(title: string): Promise<ChatSession> {
        const response = await api.post<ChatSession>('/chat/sessions', { title });
        return response.data;
    },

    // 2. –ë“Ø—Ö session-—É—É–¥—ã–≥ –∞–≤–∞—Ö
    async getSessions(): Promise<ChatSessionListResponse> {
        const response = await api.get<ChatSessionListResponse>('/chat/sessions');
        return response.data;
    },

    // 3. Session-–∏–π —Ç“Ø“Ø—Ö–∏–π–≥ –∞–≤–∞—Ö
    async getSessionHistory(sessionId: string): Promise<ChatHistoryResponse> {
        const response = await api.get<ChatHistoryResponse>(`/chat/sessions/${sessionId}`);
        return response.data;
    },

    // 4. –ú–µ—Å—Å–µ–∂ –∏–ª–≥—ç—ç—Ö
    async sendMessage(sessionId: string, content: string): Promise<Message> {
        const response = await api.post<Message>(`/chat/sessions/${sessionId}/messages`, {
            session_id: sessionId,
            content,
        });
        return response.data;
    },
};
```

---

## üîß Backend: Chat Router

### –ë–∞–π—Ä—à–∏–ª
[chat.py](file:///Users/soonko/Documents/Dentsv/Aero-Doc-AI/backend/app/routers/chat.py)

### API Endpoints

#### 1. Create Session
```python
@router.post("/sessions", response_model=ChatSessionResponse)
async def create_chat_session(
    session_data: ChatSessionCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Create a new chat session"""
    return chat_service.create_session(db, current_user, session_data.title)
```

#### 2. Get Sessions
```python
@router.get("/sessions", response_model=ChatSessionListResponse)
async def get_chat_sessions(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """List all chat sessions for the current user"""
    sessions = chat_service.get_user_sessions(db, current_user)
    return ChatSessionListResponse(sessions=sessions, total=len(sessions))
```

#### 3. Get Session History
```python
@router.get("/sessions/{session_id}", response_model=ChatHistoryResponse)
async def get_chat_history(
    session_id: UUID,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Get chat history for a specific session"""
    session = chat_service.get_session(db, session_id, current_user.id)
    return ChatHistoryResponse(session=session, messages=session.messages)
```

#### 4. Send Message (RAG)
```python
@router.post("/sessions/{session_id}/messages", response_model=ChatMessageResponse)
async def send_message(
    session_id: UUID,
    message_data: ChatMessageRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Send a message to a chat session (RAG)"""
    return await chat_service.send_message(
        db, 
        current_user.id, 
        session_id, 
        message_data.content
    )
```

---

## üíº Backend: Chat Service (RAG Logic)

### –ë–∞–π—Ä—à–∏–ª
[chat_service.py](file:///Users/soonko/Documents/Dentsv/Aero-Doc-AI/backend/app/services/chat_service.py)

### Initialization

```python
class ChatService:
    def __init__(self):
        # Vector store (ChromaDB)
        self.vector_store = VectorStore()
        
        # Embeddings (HuggingFace local)
        self.embeddings = GeminiEmbeddings()
        
        # LLM (Groq Llama 3)
        self.llm = ChatGroq(
            model=settings.LLM_MODEL,  # "llama-3.3-70b-versatile"
            api_key=settings.GROQ_API_KEY,
            temperature=0.7
        )
        
        # System prompt template
        self.system_prompt = """You are Aero-Doc AI, an intelligent assistant...
        Context:
        {context}
        """
```

### RAG Pipeline: `send_message()`

–≠–Ω—ç —Ñ—É–Ω–∫—Ü –Ω—å —á–∞—Ç—ã–Ω “Ø–Ω–¥—Å—ç–Ω logic —é–º. 7 “Ø–µ —à–∞—Ç—Ç–∞–π:

#### **–®–∞—Ç 1: Session Validation**

```python
# Session –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞—Ö
session = self.get_session(db, session_id, user_id)
```

#### **–®–∞—Ç 2: Save User Message**

```python
# –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –º–µ—Å—Å–µ–∂–∏–π–≥ database-–¥ —Ö–∞–¥–≥–∞–ª–∞—Ö
user_msg = ChatMessage(
    session_id=session_id,
    role="user",
    content=content
)
db.add(user_msg)
db.commit()
```

#### **–®–∞—Ç 3: Query Embedding**

```python
# –ê—Å—É—É–ª—Ç—ã–≥ 384-dimension vector –±–æ–ª–≥–æ—Ö
query_embedding = self.embeddings.embed_query(content)
# Result: [0.123, -0.456, 0.789, ...] (384 —Ç–æ–æ)
```

**Model**: `sentence-transformers/all-MiniLM-L6-v2`
- –õ–æ–∫–∞–ª –¥—ç—ç—Ä –∞–∂–∏–ª–ª–∞–¥–∞–≥
- –•—É—Ä–¥–∞–Ω (CPU –¥—ç—ç—Ä —á —Å–∞–π–Ω)
- 384-dimension output

#### **–®–∞—Ç 4: Vector Similarity Search**

```python
# ChromaDB-—Å —Ö–∞–º–≥–∏–π–Ω –æ–π—Ä–æ–ª—Ü–æ–æ 5 chunk-–≥ –æ–ª–æ—Ö
search_results = self.vector_store.search(
    query_embedding=query_embedding,
    top_k=5  # –î—ç—ç–¥ —Ç–∞–ª –Ω—å 5 “Ø—Ä –¥“Ø–Ω
)
```

**ChromaDB Search:**
- Cosine similarity –∞—à–∏–≥–ª–∞–Ω–∞
- Top-5 —Ö–∞–º–≥–∏–π–Ω –æ–π—Ä–æ–ª—Ü–æ–æ chunk-—É—É–¥—ã–≥ –±—É—Ü–∞–∞–Ω–∞
- Metadata (filename, chunk_index, –≥—ç—Ö –º—ç—Ç) –±–∞—Å –∏—Ä–Ω—ç

**Search Results Format:**
```python
{
    "ids": ["doc1_0", "doc1_5", "doc2_3", ...],
    "documents": ["chunk text 1", "chunk text 2", ...],
    "metadatas": [
        {"doc_id": "uuid1", "filename": "manual.pdf", "chunk_index": 0},
        {"doc_id": "uuid1", "filename": "manual.pdf", "chunk_index": 5},
        ...
    ],
    "distances": [0.12, 0.18, 0.25, ...]  # –ë–∞–≥–∞ = –æ–π—Ä
}
```

#### **–®–∞—Ç 5: Context Preparation**

```python
# –û–ª–¥—Å–æ–Ω chunk-—É—É–¥—ã–≥ –Ω—ç–≥—Ç–≥—ç—Ö
context_parts = []
sources = []

docs = search_results.get("documents", [])
metas = search_results.get("metadatas", [])

for i, doc_text in enumerate(docs):
    meta = metas[i] if i < len(metas) else {}
    filename = meta.get("filename", "Unknown")
    
    # Context string “Ø“Ø—Å–≥—ç—Ö
    context_parts.append(f"Source: {filename}\nContent: {doc_text}")
    
    # Source metadata —Ö–∞–¥–≥–∞–ª–∞—Ö
    source_entry = {
        "doc_id": meta.get("doc_id"),
        "filename": filename,
        "chunk_index": meta.get("chunk_index"),
        "page_number": meta.get("page_number")
    }
    if source_entry not in sources:
        sources.append(source_entry)

# –ë“Ø—Ö context-–∏–π–≥ –Ω—ç–≥—Ç–≥—ç—Ö
context_str = "\n\n".join(context_parts)
```

**Context Example:**
```
Source: installation_manual.pdf
Content: To install the software, first ensure that you have...

Source: user_guide.pdf
Content: The main dashboard provides access to all features...

Source: installation_manual.pdf
Content: System requirements include Python 3.8 or higher...
```

#### **–®–∞—Ç 6: LLM Response Generation**

```python
# Prompt –±—ç–ª—Ç–≥—ç—Ö
messages = [
    SystemMessage(content=self.system_prompt.format(context=context_str)),
    HumanMessage(content=content)
]

# Groq API –¥—É—É–¥–∞—Ö (Llama 3)
response = self.llm.invoke(messages)
answer_text = response.content
```

**LLM Configuration:**
- **Model**: `llama-3.3-70b-versatile`
- **Temperature**: 0.7 (–∫—Ä–µ–∞—Ç–∏–≤ –±–æ–ª–æ–≤—á —Ö—è–Ω–∞–ª—Ç—Ç–∞–π)
- **Provider**: Groq (–º–∞—à —Ö—É—Ä–¥–∞–Ω inference)

**System Prompt:**
```
You are Aero-Doc AI, an intelligent assistant designed to help users 
understand their technical documents.

Use the following pieces of retrieved context to answer the user's question.

Guidelines:
1. Base your answer ONLY on the provided context.
2. If the answer is not in the context, say "I cannot find the answer..."
3. Cite the source document filenames when possible.
4. Keep answers concise and professional.

Context:
[–æ–ª–¥—Å–æ–Ω chunk-—É—É–¥ —ç–Ω–¥ –æ—Ä–Ω–æ]
```

#### **–®–∞—Ç 7: Save Assistant Response**

```python
# AI —Ö–∞—Ä–∏—É–ª—Ç—ã–≥ database-–¥ —Ö–∞–¥–≥–∞–ª–∞—Ö
assistant_msg = ChatMessage(
    session_id=session_id,
    role="assistant",
    content=answer_text,
    sources=sources  # JSONB format
)
db.add(assistant_msg)

# Session title —à–∏–Ω—ç—á–ª—ç—Ö (—Ö—ç—Ä—ç–≤ "New Chat" –±–æ–ª)
if session.title == "New Chat":
    session.title = content[:30] + "..."

db.commit()
db.refresh(assistant_msg)

return assistant_msg
```

---

## üî¢ Vector Store Service

### –ë–∞–π—Ä—à–∏–ª
[vector_store.py](file:///Users/soonko/Documents/Dentsv/Aero-Doc-AI/backend/app/services/vector_store.py)

### ChromaDB Configuration

```python
class VectorStore:
    def __init__(self):
        # ChromaDB client
        self.client = chromadb.HttpClient(
            host=settings.CHROMA_HOST,  # "localhost"
            port=settings.CHROMA_PORT,  # 8000
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Collection “Ø“Ø—Å–≥—ç—Ö/–∞–≤–∞—Ö
        self.collection_name = "technical_documents"
        self.collection = self._get_or_create_collection()
```

### Vector Search

```python
def search(
    self,
    query_embedding: List[float],
    top_k: int = 5,
    filter_metadata: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Similarity search using cosine distance
    """
    results = self.collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k,
        where=filter_metadata  # Optional filter (e.g., user_id)
    )
    
    return {
        "ids": results['ids'][0] if results['ids'] else [],
        "documents": results['documents'][0] if results['documents'] else [],
        "metadatas": results['metadatas'][0] if results['metadatas'] else [],
        "distances": results['distances'][0] if results['distances'] else []
    }
```

**Cosine Similarity:**
- 0.0 = –Ø–≥ –∏–∂–∏–ª
- 1.0 = –û–≥—Ç ”©”©—Ä
- –ë–∞–≥–∞ —É—Ç–≥–∞ = –ò–ª“Ø“Ø –æ–π—Ä–æ–ª—Ü–æ–æ

---

## ü§ñ Embeddings Service

### –ë–∞–π—Ä—à–∏–ª
[embeddings.py](file:///Users/soonko/Documents/Dentsv/Aero-Doc-AI/backend/app/utils/embeddings.py)

### HuggingFace Local Embeddings

```python
from langchain_huggingface import HuggingFaceEmbeddings

class GeminiEmbeddings:  # –ù—ç—Ä –Ω—å —Ö—É—É—á–∏–Ω, —Ö–∞—Ä–∏–Ω HuggingFace –∞—à–∏–≥–ª–∞–∂ –±–∞–π–Ω–∞
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
    
    def embed_query(self, text: str) -> List[float]:
        """Single query embedding"""
        return self.embeddings.embed_query(text)
    
    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """Batch embedding for multiple texts"""
        return self.embeddings.embed_documents(texts)
```

**Model Specs:**
- **Name**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimension**: 384
- **Speed**: –ú–∞—à —Ö—É—Ä–¥–∞–Ω (CPU –¥—ç—ç—Ä —á —Å–∞–π–Ω)
- **Quality**: –ï—Ä”©–Ω—Ö–∏–π –∑–æ—Ä–∏—É–ª–∞–ª—Ç—ã–Ω semantic search-–¥ —Ç–æ—Ö–∏—Ä–æ–º–∂—Ç–æ–π

---

## üìä –ë“Ø—Ç—ç–Ω “Æ–π–ª –Ø–≤—Ü—ã–Ω –ñ–∏—à—ç—ç

### –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –ê—Å—É—É–ª—Ç
```
"How do I install the software?"
```

### 1. Query Embedding
```python
query_embedding = [0.123, -0.456, 0.789, ..., 0.234]  # 384 —Ç–æ–æ
```

### 2. Vector Search Results
```python
{
    "documents": [
        "To install the software, first ensure Python 3.8+ is installed...",
        "Installation steps: 1. Download the installer 2. Run setup.exe...",
        "System requirements: Windows 10, 4GB RAM, 2GB disk space..."
    ],
    "metadatas": [
        {"filename": "installation_guide.pdf", "chunk_index": 0},
        {"filename": "quick_start.pdf", "chunk_index": 2},
        {"filename": "installation_guide.pdf", "chunk_index": 5}
    ],
    "distances": [0.15, 0.22, 0.28]
}
```

### 3. Context Prepared
```
Source: installation_guide.pdf
Content: To install the software, first ensure Python 3.8+ is installed...

Source: quick_start.pdf
Content: Installation steps: 1. Download the installer 2. Run setup.exe...

Source: installation_guide.pdf
Content: System requirements: Windows 10, 4GB RAM, 2GB disk space...
```

### 4. LLM Prompt
```
System: You are Aero-Doc AI...
Context: [–¥—ç—ç—Ä—Ö context]

User: How do I install the software?
```

### 5. AI Response
```
To install the software, follow these steps:

1. Ensure you have Python 3.8 or higher installed on your system
2. Download the installer from the official website
3. Run setup.exe and follow the installation wizard
4. Verify system requirements: Windows 10, 4GB RAM, 2GB disk space

Source: installation_guide.pdf, quick_start.pdf
```

### 6. Saved to Database
```sql
-- User message
INSERT INTO chat_messages (session_id, role, content) 
VALUES ('session-uuid', 'user', 'How do I install the software?');

-- Assistant message
INSERT INTO chat_messages (session_id, role, content, sources) 
VALUES (
    'session-uuid', 
    'assistant', 
    'To install the software, follow these steps...',
    '[{"filename": "installation_guide.pdf", ...}, ...]'::jsonb
);
```

---

## üéØ RAG Pipeline-—ã–Ω –î–∞–≤—É—É –¢–∞–ª

### 1. **Context-Aware Answers**
- –ó”©–≤—Ö”©–Ω upload —Ö–∏–π—Å—ç–Ω –±–∞—Ä–∏–º—Ç–∞–∞—Å —Ö–∞—Ä–∏—É–ª–Ω–∞
- –•—É—É—Ä–∞–º—á –º—ç–¥—ç—ç–ª—ç–ª “Ø“Ø—Å–≥—ç—Ö–≥“Ø–π

### 2. **Source Citation**
- –•–∞—Ä–∏—É–ª—Ç –±“Ø—Ä —ç—Ö —Å—É—Ä–≤–∞–ª–∂—Ç–∞–π
- –•—ç—Ä—ç–≥–ª—ç–≥—á –±–∞—Ä–∏–º—Ç —Ä—É—É –æ—á–∏–∂ —à–∞–ª–≥–∞–∂ –±–æ–ª–Ω–æ

### 3. **Fast & Free**
- Groq: –î—ç–ª—Ö–∏–π–Ω —Ö–∞–º–≥–∏–π–Ω —Ö—É—Ä–¥–∞–Ω LLM inference
- Local embeddings: API –∑–∞—Ä–¥–∞–ª –±–∞–π—Ö–≥“Ø–π
- ChromaDB: “Æ–Ω—ç–≥“Ø–π vector database

### 4. **Privacy**
- –ë“Ø—Ö embedding –ª–æ–∫–∞–ª –¥—ç—ç—Ä “Ø“Ø—Å–Ω—ç
- –ó”©–≤—Ö”©–Ω LLM generation-–¥ Groq API –∞—à–∏–≥–ª–∞–Ω–∞
- –ë–∞—Ä–∏–º—Ç—É—É–¥ —Ç–∞–Ω—ã —Å–µ—Ä–≤–µ—Ä –¥—ç—ç—Ä —Ö–∞–¥–≥–∞–ª–∞–≥–¥–∞–Ω–∞

---

## üîí –ê—é—É–ª–≥“Ø–π –ë–∞–π–¥–∞–ª

### 1. Authentication
- JWT token —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π
- `get_current_user` dependency

### 2. User Isolation
- Session –±–æ–ª–æ–Ω message –±“Ø—Ä user_id-—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ–Ω–æ
- –•—ç—Ä—ç–≥–ª—ç–≥—á –∑”©–≤—Ö”©–Ω ”©”©—Ä–∏–π–Ω —á–∞—Ç—ã–≥ —Ö–∞—Ä–Ω–∞

### 3. Input Validation
- Pydantic schemas –∞—à–∏–≥–ª–∞–Ω–∞
- SQL injection prevention (SQLAlchemy ORM)

### 4. Error Handling
```python
try:
    # RAG pipeline
    ...
except Exception as e:
    # Error message —Ö–∞–¥–≥–∞–ª–∞—Ö
    assistant_msg = ChatMessage(
        session_id=session_id,
        role="assistant",
        content="I encountered an error...",
        sources=[{"error": str(e)}]
    )
    db.add(assistant_msg)
    db.commit()
    raise HTTPException(status_code=500, detail=error_msg)
```

---

## ‚öôÔ∏è Configuration

### Backend Config
[config.py](file:///Users/soonko/Documents/Dentsv/Aero-Doc-AI/backend/app/config.py)

```python
# RAG Configuration
CHUNK_SIZE: int = 1000
CHUNK_OVERLAP: int = 200
TOP_K_RESULTS: int = 5

# LLM Configuration
LLM_MODEL: str = "llama-3.3-70b-versatile"
EMBEDDING_MODEL: str = "sentence-transformers/all-MiniLM-L6-v2"
LLM_TEMPERATURE: float = 0.7
MAX_OUTPUT_TOKENS: int = 2048

# ChromaDB
CHROMA_HOST: str = "localhost"
CHROMA_PORT: int = 8000
```

---

## üìù –•—É—Ä–∞–∞–Ω–≥—É–π

### Chat Flow (7 –®–∞—Ç)

```
1. User Message ‚Üí Database
2. Query ‚Üí Embedding (384-dim vector)
3. Vector Search ‚Üí ChromaDB (top-5 chunks)
4. Context Preparation ‚Üí Format results
5. LLM Prompt ‚Üí System + Context + Question
6. Groq API ‚Üí Generate answer
7. Assistant Message ‚Üí Database (with sources)
```

### –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–π–Ω Stack

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –¢–µ—Ö–Ω–æ–ª–æ–≥–∏ | “Æ“Ø—Ä—ç–≥ |
|-----------|-----------|-------|
| **Frontend** | React/Next.js | Chat UI, optimistic updates |
| **API** | FastAPI | REST endpoints, JWT auth |
| **Database** | PostgreSQL | Sessions, messages storage |
| **Embeddings** | HuggingFace (local) | Text ‚Üí 384-dim vectors |
| **Vector DB** | ChromaDB | Similarity search |
| **LLM** | Groq (Llama 3) | Answer generation |

### –î–∞–≤—É—É –¢–∞–ª—É—É–¥

‚úÖ **–•—É—Ä–¥–∞–Ω**: Groq-–∏–π–Ω LPU —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏  
‚úÖ **“Æ–Ω—ç–≥“Ø–π**: Local embeddings, ChromaDB  
‚úÖ **–ù–∞—Ä–∏–π–≤—á–ª–∞–ª—Ç–∞–π**: Context-based answers  
‚úÖ **–≠—Ö —Å—É—Ä–≤–∞–ª–∂—Ç–∞–π**: Source citation  
‚úÖ **–ê—é—É–ª–≥“Ø–π**: User isolation, JWT auth  
‚úÖ **–ú–∞—Å—à—Ç–∞–±–ª–∞–≥–¥–∞—Ö**: Vector DB, async processing  

---
