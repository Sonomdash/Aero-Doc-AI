# Text Embedding: –ì“Ø–Ω–∑–≥–∏–π –¢–∞–π–ª–±–∞—Ä

–≠–Ω—ç –±–∞—Ä–∏–º—Ç –Ω—å `query_embedding = self.embeddings.embed_query(content)` –∫–æ–¥—ã–Ω –º”©—Ä–∏–π–≥ –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç–∞–π–ª–±–∞—Ä–ª–∞–Ω–∞.

---

## üéØ Embedding –≥—ç–∂ —é—É –≤—ç?

**Embedding** –≥—ç–¥—ç–≥ –Ω—å —Ç–µ–∫—Å—Ç–∏–π–≥ —Ç–æ–æ–Ω –≤–µ–∫—Ç–æ—Ä (–∂–∞–≥—Å–∞–∞–ª—Ç) –±–æ–ª–≥–æ—Ö “Ø–π–ª —è–≤—Ü —é–º.

### –ñ–∏—à—ç—ç

**–¢–µ–∫—Å—Ç:**
```
"How do I install the software?"
```

**Embedding (384-dimension vector):**
```python
[0.123, -0.456, 0.789, 0.234, -0.567, ..., 0.891]
# –ù–∏–π—Ç 384 —Ç–æ–æ
```

---

## ü§î –Ø–∞–≥–∞–∞–¥ Embedding –•—ç—Ä—ç–≥—Ç—ç–π –≤—ç?

### –ê—Å—É—É–¥–∞–ª: –ö–æ–º–ø—å—é—Ç–µ—Ä —Ç–µ–∫—Å—Ç–∏–π–≥ –æ–π–ª–≥–æ—Ö–≥“Ø–π

–ö–æ–º–ø—å—é—Ç–µ—Ä –∑”©–≤—Ö”©–Ω —Ç–æ–æ–≥–æ–æ—Ä –∞–∂–∏–ª–ª–∞–¥–∞–≥. –¢–µ–∫—Å—Ç–∏–π–≥ —à—É—É–¥ —Ö–∞—Ä—å—Ü—É—É–ª–∂, —É—Ç–≥–∞ —Å–∞–Ω–∞–∞–≥ –Ω—å –æ–π–ª–≥–æ–∂ —á–∞–¥–∞—Ö–≥“Ø–π.

**–ñ–∏—à—ç—ç:**
```python
# –≠–¥–≥—ç—ç—Ä –∞—Å—É—É–ª—Ç—É—É–¥ –∏–∂–∏–ª —É—Ç–≥–∞—Ç–∞–π –±–æ–ª–æ–≤—á —Ç–µ–∫—Å—Ç ”©”©—Ä
query1 = "How do I install the software?"
query2 = "What are the installation steps?"
query3 = "Software installation guide"

# –ö–æ–º–ø—å—é—Ç–µ—Ä —ç–¥–≥—ç—ç—Ä–∏–π–≥ —è–∞–∂ –∏–∂–∏–ª –≥—ç–∂ –º—ç–¥—ç—Ö –≤—ç?
# ‚Üí Embedding –∞—à–∏–≥–ª–∞–Ω–∞!
```

### –®–∏–π–¥—ç–ª: Semantic Vector Space

Embedding –Ω—å —Ç–µ–∫—Å—Ç–∏–π–≥ **—É—Ç–≥–∞ —Å–∞–Ω–∞–∞–Ω—ã –æ—Ä–æ–Ω –∑–∞–π** (semantic space) –¥—ç—ç—Ä –±–∞–π—Ä–ª—É—É–ª–Ω–∞.

```
                    ‚Üë
                    ‚îÇ
    "install" ‚óè     ‚îÇ     ‚óè "setup"
                    ‚îÇ
    "software" ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚óè "application"
                    ‚îÇ
                    ‚îÇ     ‚óè "program"
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
                    ‚îÇ
    "delete" ‚óè      ‚îÇ
                    ‚îÇ
                    ‚Üì
```

**–ò–∂–∏–ª —É—Ç–≥–∞—Ç–∞–π “Ø–≥—Å** ‚Üí –û–π—Ä–æ–ª—Ü–æ–æ –±–∞–π—Ä–ª–∞–Ω–∞  
**”®”©—Ä —É—Ç–≥–∞—Ç–∞–π “Ø–≥—Å** ‚Üí –•–æ–ª–¥–æ–Ω–æ

---

## üß† Embedding –•—ç—Ä—Ö—ç–Ω “Æ“Ø—Å–¥—ç–≥ –≤—ç?

### 1. Neural Network Model

–¢–∞–Ω—ã —Å–∏—Å—Ç–µ–º–¥ **`sentence-transformers/all-MiniLM-L6-v2`** model –∞—à–∏–≥–ª–∞–¥–∞–≥.

#### Model Architecture

```
Input Text
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tokenization       ‚îÇ  ‚Üê “Æ–≥ –±“Ø—Ä–∏–π–≥ —Ç–æ–æ –±–æ–ª–≥–æ—Ö
‚îÇ  "install" ‚Üí 5234   ‚îÇ
‚îÇ  "software" ‚Üí 8901  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BERT-based         ‚îÇ  ‚Üê Neural network (6 –¥–∞–≤—Ö–∞—Ä–≥–∞)
‚îÇ  Transformer        ‚îÇ
‚îÇ  (6 layers)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Mean Pooling       ‚îÇ  ‚Üê –î—É–Ω–¥–∞–∂ –∞–≤–∞—Ö
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Normalization      ‚îÇ  ‚Üê Normalize —Ö–∏–π—Ö
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
384-dimension Vector
[0.123, -0.456, ...]
```

### 2. –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –ê–ª—Ö–º—É—É–¥

#### –ê–ª—Ö–∞–º 1: Tokenization (“Æ–≥ –∑–∞–¥–ª–∞—Ö)

```python
# Input
text = "How do I install the software?"

# Tokenization
tokens = ["How", "do", "I", "install", "the", "software", "?"]

# Token IDs (vocabulary-—Å)
token_ids = [2129, 2079, 1045, 16500, 1996, 4007, 1029]
```

#### –ê–ª—Ö–∞–º 2: Transformer Neural Network

Transformer model –Ω—å “Ø–≥ –±“Ø—Ä–∏–π–Ω **–∫–æ–Ω—Ç–µ–∫—Å—Ç** (”©–º–Ω”©—Ö –±–æ–ª–æ–Ω –¥–∞—Ä–∞–∞–≥–∏–π–Ω “Ø–≥—Å)-–∏–π–≥ –æ–π–ª–≥–æ–Ω–æ.

```python
# –ñ–∏—à—ç—ç: "install" –≥—ç–¥—ç–≥ “Ø–≥
# –ö–æ–Ω—Ç–µ–∫—Å—Ç: "How do I [install] the software?"

# Transformer-–∏–π–Ω –≥–∞—Ä–∞–ª—Ç:
# "install" ‚Üí [0.5, -0.3, 0.8, ..., 0.2]  (768-dim)
# –≠–Ω—ç –≤–µ–∫—Ç–æ—Ä –Ω—å "install software" –≥—ç—Å—ç–Ω —É—Ç–≥—ã–≥ –∞–≥—É—É–ª–Ω–∞
```

**Transformer-–∏–π–Ω –æ–Ω—Ü–ª–æ–≥:**
- **Self-Attention**: “Æ–≥ –±“Ø—Ä –±—É—Å–∞–¥ “Ø–≥—Å—Ç—ç–π —Ö—ç—Ä—Ö—ç–Ω —Ö–æ–ª–±–æ–≥–¥–æ—Ö—ã–≥ —Ç–æ–æ—Ü–æ–æ–ª–Ω–æ
- **Context-Aware**: –ò–∂–∏–ª “Ø–≥ ”©”©—Ä ”©–≥“Ø“Ø–ª–±—ç—Ä—Ç ”©”©—Ä —É—Ç–≥–∞—Ç–∞–π –±–∞–π–∂ –±–æ–ª–Ω–æ

–ñ–∏—à—ç—ç:
```
"Apple is a fruit" ‚Üí "Apple" = [0.1, 0.9, ...]
"Apple is a company" ‚Üí "Apple" = [0.8, 0.2, ...]
```

#### –ê–ª—Ö–∞–º 3: Mean Pooling (–î—É–Ω–¥–∞–∂)

Transformer –Ω—å “Ø–≥ –±“Ø—Ä—Ç –≤–µ–∫—Ç–æ—Ä “Ø“Ø—Å–≥—ç–Ω—ç. –ë“Ø—Ö ”©–≥“Ø“Ø–ª–±—ç—Ä–∏–π–≥ –Ω—ç–≥ –≤–µ–∫—Ç–æ—Ä –±–æ–ª–≥–æ—Ö—ã–Ω —Ç—É–ª–¥ –¥—É–Ω–¥–∞–∂ –∞–≤–Ω–∞.

```python
# “Æ–≥ –±“Ø—Ä–∏–π–Ω –≤–µ–∫—Ç–æ—Ä
word_vectors = [
    [0.5, -0.3, 0.8],  # "How"
    [0.2, 0.4, -0.1],  # "do"
    [0.1, 0.6, 0.3],   # "I"
    [0.7, -0.2, 0.5],  # "install"
    # ...
]

# Mean pooling (–¥—É–Ω–¥–∞–∂)
sentence_vector = mean(word_vectors)
# = [(0.5+0.2+0.1+0.7)/4, (-0.3+0.4+0.6-0.2)/4, ...]
# = [0.375, 0.125, ...]
```

#### –ê–ª—Ö–∞–º 4: Normalization

–í–µ–∫—Ç–æ—Ä —É—Ä—Ç –Ω—å 1 –±–æ–ª–≥–æ—Ö (unit vector).

```python
# ”®–º–Ω”©
vector = [0.375, 0.125, 0.625]
length = sqrt(0.375¬≤ + 0.125¬≤ + 0.625¬≤) = 0.738

# Normalization
normalized = [0.375/0.738, 0.125/0.738, 0.625/0.738]
           = [0.508, 0.169, 0.847]

# –û–¥–æ–æ length = 1
```

**–Ø–∞–≥–∞–∞–¥ normalize —Ö–∏–π—Ö –≤—ç?**
- Cosine similarity —Ç–æ–æ—Ü–æ–æ–ª–æ—Ö–æ–¥ —Ö—è–ª–±–∞—Ä
- –¢–µ–∫—Å—Ç–∏–π–Ω —É—Ä—Ç (“Ø–≥–∏–π–Ω —Ç–æ–æ) –Ω”©–ª”©”©–ª”©—Ö–≥“Ø–π

---

## üíª –ö–æ–¥—ã–Ω –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π

### –¢–∞–Ω—ã –°–∏—Å—Ç–µ–º–∏–π–Ω –ö–æ–¥

```python
# backend/app/utils/embeddings.py
from langchain_huggingface import HuggingFaceEmbeddings

class GeminiEmbeddings:
    def __init__(self):
        self.client = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    def embed_query(self, text: str) -> List[float]:
        """Generate embedding for query"""
        return self.client.embed_query(text)
```

### –î–æ—Ç–æ–æ–¥ “Æ–π–ª –Ø–≤—Ü

```python
# –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç
content = "How do I install the software?"

# 1. Model load (–∞–Ω—Ö —É–¥–∞–∞)
# - Model —Ñ–∞–π–ª—É—É–¥—ã–≥ —Ç–∞—Ç–∞–∂ –∞–≤–Ω–∞ (~90MB)
# - Memory-–¥ load —Ö–∏–π–Ω—ç

# 2. Tokenization
tokens = tokenizer.encode(content)
# ‚Üí [101, 2129, 2079, 1045, 16500, 1996, 4007, 1029, 102]
#    [CLS] How  do   I    install the  software ?   [SEP]

# 3. Transformer forward pass
# - 6 –¥–∞–≤—Ö–∞—Ä–≥–∞ neural network
# - Self-attention —Ç–æ–æ—Ü–æ–æ–ª–æ–ª
# - –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–π–ª–≥–æ—Ö
hidden_states = model(tokens)
# ‚Üí Shape: (1, 9, 384)  # (batch, tokens, hidden_dim)

# 4. Mean pooling
sentence_embedding = mean_pooling(hidden_states, attention_mask)
# ‚Üí Shape: (1, 384)

# 5. Normalization
normalized_embedding = F.normalize(sentence_embedding, p=2, dim=1)
# ‚Üí Shape: (1, 384)

# 6. –ë—É—Ü–∞–∞—Ö
query_embedding = normalized_embedding[0].tolist()
# ‚Üí [0.123, -0.456, 0.789, ..., 0.234]  # 384 —Ç–æ–æ
```

---

## üîç Similarity Search (–ò–∂–∏–ª —Ç”©—Å—Ç—ç–π —Ö–∞–π–ª—Ç)

Embedding-–∏–π–Ω –≥–æ–ª –∑–æ—Ä–∏–ª–≥–æ –Ω—å **–∏–∂–∏–ª —Ç”©—Å—Ç—ç–π —Ç–µ–∫—Å—Ç –æ–ª–æ—Ö** —è–≤–¥–∞–ª —é–º.

### Cosine Similarity

```python
# Query embedding
query = [0.5, 0.3, 0.8]

# Document embeddings (ChromaDB-–¥ —Ö–∞–¥–≥–∞–ª–∞–≥–¥—Å–∞–Ω)
doc1 = [0.6, 0.2, 0.7]  # "Installation guide"
doc2 = [0.1, 0.9, 0.2]  # "Pricing information"
doc3 = [0.5, 0.4, 0.8]  # "Setup instructions"

# Cosine similarity —Ç–æ–æ—Ü–æ–æ–ª–æ—Ö
similarity(query, doc1) = 0.95  # –ú–∞—à –æ–π—Ä
similarity(query, doc2) = 0.35  # –•–æ–ª–¥—É—É
similarity(query, doc3) = 0.98  # –•–∞–º–≥–∏–π–Ω –æ–π—Ä

# –≠—Ä—ç–º–±—ç–ª—ç—Ö
# 1. doc3 (0.98) ‚Üê –•–∞–º–≥–∏–π–Ω —Ö–æ–ª–±–æ–≥–¥–æ–ª—Ç–æ–π
# 2. doc1 (0.95)
# 3. doc2 (0.35)
```

### Cosine Similarity –¢–æ–º—ä—ë–æ

```
                    A ¬∑ B
cos(Œ∏) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          ||A|| √ó ||B||

Normalized vector –±–æ–ª:
cos(Œ∏) = A ¬∑ B  (—ç–Ω–≥–∏–π–Ω dot product)
```

**–£—Ç–≥–∞:**
- `1.0` = –Ø–≥ –∏–∂–∏–ª
- `0.0` = –Ø–º–∞—Ä —á —Ö–æ–ª–±–æ–æ–≥“Ø–π
- `-1.0` = –≠—Å—Ä—ç–≥ —É—Ç–≥–∞—Ç–∞–π

---

## üìä –ë–æ–¥–∏—Ç –ñ–∏—à—ç—ç

### Input: –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –ê—Å—É—É–ª—Ç

```python
query = "How do I reset my password?"
```

### Embedding Process

```python
# 1. Tokenization
tokens = ["How", "do", "I", "reset", "my", "password", "?"]

# 2. Transformer (simplified)
# “Æ–≥ –±“Ø—Ä–∏–π–Ω –≤–µ–∫—Ç–æ—Ä (768-dim, –¥–∞—Ä–∞–∞ –Ω—å 384 –±–æ–ª–≥–æ–Ω–æ)
embeddings = {
    "How": [0.1, 0.2, ..., 0.3],
    "do": [0.4, -0.1, ..., 0.2],
    "I": [0.2, 0.3, ..., -0.1],
    "reset": [0.8, 0.5, ..., 0.6],      # ‚Üê –ì–æ–ª “Ø–≥
    "my": [0.1, 0.1, ..., 0.0],
    "password": [0.7, 0.6, ..., 0.5],   # ‚Üê –ì–æ–ª “Ø–≥
    "?": [0.0, 0.0, ..., 0.0]
}

# 3. Mean pooling
sentence_embedding = average(embeddings.values())

# 4. Final embedding
query_embedding = [0.329, 0.271, ..., 0.214]  # 384 —Ç–æ–æ
```

### ChromaDB Search

```python
# ChromaDB-–¥ —Ö–∞–¥–≥–∞–ª–∞–≥–¥—Å–∞–Ω –±–∞—Ä–∏–º—Ç—ã–Ω chunk-—É—É–¥
chunks = [
    {
        "text": "To reset your password, go to Settings > Security",
        "embedding": [0.325, 0.268, ..., 0.219]  # –û–π—Ä–æ–ª—Ü–æ–æ!
    },
    {
        "text": "The installation process takes 5 minutes",
        "embedding": [0.112, 0.891, ..., 0.334]  # –•–æ–ª–¥—É—É
    },
    {
        "text": "Password reset instructions: click Forgot Password",
        "embedding": [0.331, 0.275, ..., 0.211]  # –ú–∞—à –æ–π—Ä!
    }
]

# Similarity —Ç–æ–æ—Ü–æ–æ–ª–æ—Ö
similarities = [
    cosine_similarity(query_embedding, chunks[0]["embedding"]),  # 0.97
    cosine_similarity(query_embedding, chunks[1]["embedding"]),  # 0.23
    cosine_similarity(query_embedding, chunks[2]["embedding"]),  # 0.99
]

# Top-5 —Å–æ–Ω–≥–æ—Ö
top_results = [chunks[2], chunks[0]]  # –•–∞–º–≥–∏–π–Ω –æ–π—Ä–æ–ª—Ü–æ–æ 2
```

---

## üé® Visualization: Semantic Space

Embedding-–∏–π–≥ 2D –¥—ç—ç—Ä –¥“Ø—Ä—Å—ç–ª–±—ç–ª:

```
                    ‚Üë password
                    ‚îÇ
    "reset pwd" ‚óè   ‚îÇ   ‚óè "change password"
                    ‚îÇ
    "forgot pwd" ‚óè‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚óè "password recovery"
                    ‚îÇ
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí installation
                    ‚îÇ
                    ‚îÇ   ‚óè "install software"
    "setup app" ‚óè   ‚îÇ
                    ‚îÇ   ‚óè "installation guide"
                    ‚Üì
```

**–ò–∂–∏–ª —É—Ç–≥–∞—Ç–∞–π –∞—Å—É—É–ª—Ç—É—É–¥ –æ–π—Ä–æ–ª—Ü–æ–æ –±–∞–π—Ä–ª–∞–Ω–∞:**
- "How do I reset my password?"
- "Forgot my password"
- "Password recovery steps"

**”®”©—Ä —É—Ç–≥–∞—Ç–∞–π –∞—Å—É—É–ª—Ç—É—É–¥ —Ö–æ–ª–¥–æ–Ω–æ:**
- "How to install software?"
- "System requirements"

---

## ‚ö° Performance & Optimization

### Model Specs

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –£—Ç–≥–∞ |
|----------|------|
| **Model Name** | all-MiniLM-L6-v2 |
| **Parameters** | 22.7M (22.7 —Å–∞—è) |
| **Layers** | 6 (BERT-based) |
| **Hidden Size** | 384 |
| **Model Size** | ~90MB |
| **Speed (CPU)** | ~100 sentences/sec |
| **Speed (GPU)** | ~1000 sentences/sec |

### –Ø–∞–≥–∞–∞–¥ MiniLM-L6-v2 –≤—ç?

‚úÖ **–•—É—Ä–¥–∞–Ω**: –ñ–∏–∂–∏–≥ model (6 –¥–∞–≤—Ö–∞—Ä–≥–∞)  
‚úÖ **–ß–∞–Ω–∞—Ä—Ç–∞–π**: Semantic search-–¥ —Å–∞–π–Ω “Ø—Ä –¥“Ø–Ω—Ç—ç–π  
‚úÖ **CPU-friendly**: GPU —à–∞–∞—Ä–¥–ª–∞–≥–∞–≥“Ø–π  
‚úÖ **“Æ–Ω—ç–≥“Ø–π**: –õ–æ–∫–∞–ª –¥—ç—ç—Ä –∞–∂–∏–ª–ª–∞–Ω–∞  
‚úÖ **–ë–∞–≥—Ç–∞–∞–º–∂—Ç–∞–π**: 90MB –∑–∞–π —ç–∑—ç–ª–Ω—ç  

### –ë—É—Å–∞–¥ Model-—Ç—ç–π –•–∞—Ä—å—Ü—É—É–ª–±–∞–ª

| Model | Layers | Dim | Size | Speed | Quality |
|-------|--------|-----|------|-------|---------|
| **all-MiniLM-L6-v2** | 6 | 384 | 90MB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| all-mpnet-base-v2 | 12 | 768 | 420MB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| OpenAI text-embedding-3 | - | 1536 | API | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üéØ –•—É—Ä–∞–∞–Ω–≥—É–π

### `embed_query()` –Æ—É –•–∏–π–¥—ç–≥ –≤—ç?

```python
query_embedding = self.embeddings.embed_query(content)
```

**–≠–Ω–≥–∏–π–Ω —Ö—ç–ª–ª—ç–≥—ç—ç—Ä:**
> –¢–µ–∫—Å—Ç–∏–π–≥ 384 —Ç–æ–æ–Ω—ã –∂–∞–≥—Å–∞–∞–ª—Ç –±–æ–ª–≥–æ–Ω–æ. –≠–Ω—ç –∂–∞–≥—Å–∞–∞–ª—Ç –Ω—å —Ç–µ–∫—Å—Ç–∏–π–Ω **—É—Ç–≥–∞ —Å–∞–Ω–∞–∞**-–≥ –∏–ª—ç—Ä—Ö–∏–π–ª–Ω—ç.

**–¢–µ—Ö–Ω–∏–∫–∏–π–Ω —Ö—ç–ª–ª—ç–≥—ç—ç—Ä:**
> Neural network (Transformer) –∞—à–∏–≥–ª–∞–Ω —Ç–µ–∫—Å—Ç–∏–π–≥ semantic vector space –¥—ç—ç—Ä –±–∞–π—Ä–ª—É—É–ª–Ω–∞. –ò–∂–∏–ª —É—Ç–≥–∞—Ç–∞–π —Ç–µ–∫—Å—Ç“Ø“Ø–¥ –æ–π—Ä–æ–ª—Ü–æ–æ, ”©”©—Ä —É—Ç–≥–∞—Ç–∞–π —Ç–µ–∫—Å—Ç“Ø“Ø–¥ —Ö–æ–ª–¥–æ–Ω–æ.

### –ê–ª—Ö–∞–º –±“Ø—Ä–∏–π–Ω —Ö—É—Ä–∞–∞–Ω–≥—É–π

1. **Tokenization**: “Æ–≥ ‚Üí –¢–æ–æ
2. **Transformer**: –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–π–ª–≥–æ—Ö (6 –¥–∞–≤—Ö–∞—Ä–≥–∞ neural network)
3. **Mean Pooling**: “Æ–≥ –±“Ø—Ä–∏–π–Ω –≤–µ–∫—Ç–æ—Ä ‚Üí ”®–≥“Ø“Ø–ª–±—ç—Ä–∏–π–Ω –≤–µ–∫—Ç–æ—Ä
4. **Normalization**: –£—Ä—Ç = 1 –±–æ–ª–≥–æ—Ö
5. **Output**: 384-dimension vector

### –Ø–∞–≥–∞–∞–¥ –≠–Ω—ç –ß—É—Ö–∞–ª –≤—ç?

‚úÖ **Semantic Search**: “Æ–≥ –±“Ø—Ä —Ç–∞–∞—Ä–∞—Ö–≥“Ø–π —á —É—Ç–≥–∞ –Ω—å –∏–∂–∏–ª –±–æ–ª –æ–ª–Ω–æ  
‚úÖ **Context-Aware**: –ò–∂–∏–ª “Ø–≥ ”©”©—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—ç–¥ ”©”©—Ä —É—Ç–≥–∞—Ç–∞–π  
‚úÖ **Fast**: –õ–æ–∫–∞–ª –¥—ç—ç—Ä —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª–∞–Ω–∞  
‚úÖ **Free**: API –∑–∞—Ä–¥–∞–ª –±–∞–π—Ö–≥“Ø–π  

---
